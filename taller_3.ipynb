{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQ-VfNtOyJbsaxu43Kztf_cv1mgBG6ZIQZEVw&usqp=CAU'>\n",
    "\n",
    "# Procesamiento de Lenguaje Natural\n",
    "\n",
    "## Taller #3: Web Scraping\n",
    "`Fecha de entrega: Marzo 11, 2021. (Antes del inicio de la próxima clase).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 1:\n",
    "\n",
    "- `[15 pts]` Hacer Web Scraping de 10 animales en Wikipedia (en búcle)\n",
    "- `[10 pts]` Obtener el **encabezado** de cada animal\n",
    "- `[15 pts]` Obtener todos los **textos** que están en las etiquetas de negrilla del primer párrafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs \n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enlace = \"https://es.wikipedia.org/wiki/Canis_familiaris\"\n",
    "enlace1 = \"https://es.wikipedia.org/wiki/Canis_lupus\"\n",
    "enlace2 = \"https://es.wikipedia.org/wiki/Delphinidae\"\n",
    "enlace3 = \"https://es.wikipedia.org/wiki/Oryctolagus_cuniculus\"\n",
    "enlace4 = \"https://es.wikipedia.org/wiki/Psittacoidea\"\n",
    "enlace5 = \"https://es.wikipedia.org/wiki/Struthio_camelus\"\n",
    "enlace6 = \"https://es.wikipedia.org/wiki/Elephantidae\"\n",
    "enlace7 = \"https://es.wikipedia.org/wiki/Gorilla\"\n",
    "enlace8 = \"https://es.wikipedia.org/wiki/Panthera_pardus\"\n",
    "enlace9 = \"https://es.wikipedia.org/wiki/Giraffa_camelopardalis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = [enlace, enlace1, enlace2, enlace3, enlace4, enlace5, enlace6, enlace7, enlace8, enlace9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = urllib.request.Request(enlace, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urllib.request.urlopen(request)\n",
    "source = webpage.read()\n",
    "webpage.close()\n",
    "soup = bs.BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "request1 = urllib.request.Request(enlace1, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage1 = urllib.request.urlopen(request1)\n",
    "source1 = webpage1.read()\n",
    "webpage1.close()\n",
    "soup1 = bs.BeautifulSoup(source1, 'html.parser')\n",
    "\n",
    "request2= urllib.request.Request(enlace2, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage2 = urllib.request.urlopen(request2)\n",
    "source2 = webpage2.read()\n",
    "webpage2.close()\n",
    "soup2 = bs.BeautifulSoup(source2, 'html.parser')\n",
    "\n",
    "request3 = urllib.request.Request(enlace3, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage3 = urllib.request.urlopen(request3)\n",
    "source3 = webpage3.read()\n",
    "webpage3.close()\n",
    "soup3 = bs.BeautifulSoup(source3, 'html.parser')\n",
    "\n",
    "request4 = urllib.request.Request(enlace4, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage4 = urllib.request.urlopen(request4)\n",
    "source4 = webpage4.read()\n",
    "webpage4.close()\n",
    "soup4 = bs.BeautifulSoup(source4, 'html.parser')\n",
    "\n",
    "request5 = urllib.request.Request(enlace5, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage5 = urllib.request.urlopen(request5)\n",
    "source5 = webpage5.read()\n",
    "webpage5.close()\n",
    "soup5 = bs.BeautifulSoup(source5, 'html.parser')\n",
    "\n",
    "request6 = urllib.request.Request(enlace6, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage6 = urllib.request.urlopen(request6)\n",
    "source6 = webpage6.read()\n",
    "webpage6.close()\n",
    "soup6 = bs.BeautifulSoup(source6, 'html.parser')\n",
    "\n",
    "request7 = urllib.request.Request(enlace7, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage7 = urllib.request.urlopen(request7)\n",
    "source7 = webpage7.read()\n",
    "webpage7.close()\n",
    "soup7 = bs.BeautifulSoup(source7, 'html.parser')\n",
    "\n",
    "request8 = urllib.request.Request(enlace8, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage8 = urllib.request.urlopen(request8)\n",
    "source8 = webpage8.read()\n",
    "webpage8.close()\n",
    "soup8 = bs.BeautifulSoup(source8, 'html.parser')\n",
    "\n",
    "request9 = urllib.request.Request(enlace9, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage9 = urllib.request.urlopen(request9)\n",
    "source9 = webpage9.read()\n",
    "webpage9.close()\n",
    "soup9 = bs.BeautifulSoup(source9, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_list = [soup, soup1, soup2, soup3, soup4, soup5, soup6, soup7, soup8, soup9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Canis familiaris']\n",
      "['Canis lupus']\n",
      "['Delphinidae']\n",
      "['Oryctolagus cuniculus']\n",
      "['Psittacoidea']\n",
      "['Struthio camelus']\n",
      "['Elephantidae']\n",
      "['Gorilla']\n",
      "['Panthera pardus']\n",
      "['Giraffa camelopardalis']\n"
     ]
    }
   ],
   "source": [
    "for i in soup_list:\n",
    "    print(i.find(\"h1\").contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'contents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-066192bb5dd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tbody\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"p\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'contents'"
     ]
    }
   ],
   "source": [
    "for i in soup_list:\n",
    "    print(i.find(\"tbody\").find(\"p\").contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gato doméstico\n",
      "Felis silvestris catus\n",
      "gato\n",
      "minino\n",
      "michino\n",
      "michi\n",
      "micho\n",
      "mizo\n",
      "miz\n",
      "morroño\n",
      "morrongo\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 2:\n",
    "- `[10 pts]` Usando regex, reemplazar todas las caracteres especiales del punto anterior (palabras en negrilla) por un asterisco (¡Ojo, los espacios se quedan!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gato dom*stico\n",
      "Felis silvestris catus\n",
      "gato\n",
      "minino\n",
      "michino\n",
      "michi\n",
      "micho\n",
      "mizo\n",
      "miz\n",
      "morro*o\n",
      "morrongo\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
